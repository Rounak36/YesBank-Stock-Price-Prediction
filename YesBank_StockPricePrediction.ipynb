{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **Stock Price Prediction using Machine Learning**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - RegressionUnsupervised\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In today’s fast-paced financial world, anticipating stock market trends is not just an advantage—it’s a necessity. This capstone project dives into the practical challenge of predicting stock prices, using Yes Bank’s historical stock data as a case study. The idea was to explore how machine learning models can help investors or financial analysts get a better understanding of market behavior and make more informed decisions\n",
        "\n",
        "The core goal was to build a predictive model that could estimate the closing stock price of Yes Bank using historical data such as Open, High, Low, and Date. Yes Bank, once one of India’s most promising private sector banks, has seen massive stock volatility over the years due to management shifts, financial instability, and market speculation. This made it an ideal candidate for this project volatile enough to challenge the model and valuable enough to matter."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stock markets are influenced by countless factors—ranging from market sentiment and economic indicators to company-specific news. For financial institutions, investors, and analysts, the ability to accurately predict future stock prices can significantly reduce risk and improve decision-making."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# For visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning tools\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Plotting setup\n",
        "%matplotlib inline\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Ignore warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "file_path = '/content/current data_YesBank_StockPrices - data_YesBank_StockPrices.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Take a quick look at the data\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "\n",
        "# Basic info about the dataset\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nColumn Names:\", df.columns.tolist())\n",
        "\n",
        "# Preview the top 5 rows\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "\n",
        "# Get the number of rows and columns\n",
        "rows, columns = df.shape\n",
        "print(f\"Total Rows: {rows}\")\n",
        "print(f\"Total Columns: {columns}\")\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()\n"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicate_count}\")\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing values in each column:\\n\")\n",
        "print(missing_values)\n"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# Bar plot of missing values (safe version)\n",
        "missing = df.isnull().sum()\n",
        "missing = missing[missing > 0]\n",
        "\n",
        "if not missing.empty:\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    missing.plot(kind='bar', color='salmon')\n",
        "    plt.title(\"Missing Values Count per Column\")\n",
        "    plt.xlabel(\"Columns\")\n",
        "    plt.ylabel(\"Missing Values\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\" No missing values found in the dataset.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original rows:\", len(df))\n"
      ],
      "metadata": {
        "id": "FwaIZp15CNhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning the Dataset"
      ],
      "metadata": {
        "id": "JfTpQt-JElTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# STEP 1: Check original structure\n",
        "print(\" Original Data Summary:\")\n",
        "print(df.head())\n",
        "print(df.columns)\n",
        "print(df.info())\n",
        "\n",
        "# STEP 2: Check for missing values\n",
        "print(\"\\n Missing Values in Key Columns:\")\n",
        "print(df[['Date', 'Open', 'High', 'Low', 'Close']].isnull().sum())\n",
        "\n",
        "# STEP 3: Check data types\n",
        "print(\"\\n Data Types Before Cleaning:\")\n",
        "print(df[['Date', 'Open', 'High', 'Low', 'Close']].dtypes)\n",
        "\n",
        "# STEP 4: Fix the Date format and clean data\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%b-%y', errors='coerce')\n",
        "\n",
        "for col in ['Open', 'High', 'Low', 'Close']:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "df_clean = df.dropna(subset=['Date', 'Open', 'High', 'Low', 'Close'])\n",
        "\n",
        "# Result\n",
        "print(\"\\n Data Cleaning Result:\")\n",
        "print(\"Original rows:\", len(df))\n",
        "print(\"Rows after cleaning:\", len(df_clean))\n",
        "\n",
        "df = df_clean.copy()\n",
        "\n"
      ],
      "metadata": {
        "id": "anOuVbA9DQ-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cleaned rows:\", len(df))\n"
      ],
      "metadata": {
        "id": "npeeog3ZCQyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains monthly stock data of Yes Bank, including Date, Open, High, Low, and Close prices. It is clean, with no missing or duplicate values, and all features are numeric except the Date, which will need conversion for time-based analysis. The goal is to predict the closing price of the stock. Overall, the dataset is well-structured and suitable for building a reliable regression model to understand and forecast stock trends"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(\"Column Names and Data Types:\\n\")\n",
        "print(df.dtypes)\n"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()\n"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vBXTPXncTMp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset includes five main variables that represent key stock price indicators for Yes Bank. The Date column tells us the month and year of each record, which helps in understanding the timeline of stock movement. The Open price is the value at which the stock began trading at the start of the month, while the High and Low columns capture the highest and lowest prices the stock reached during that period. Finally, the Close price is the most important one for us,it shows the stock’s price at the end of the month and is the value we’re trying to predict. Each of these variables gives us a different perspective on the stock’s monthly performance and together they help us analyze trends, volatility, and overall market behavior over time."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "print(\"Unique values in each column:\\n\")\n",
        "print(df.nunique())\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "# Convert 'Date' column to datetime format\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "# Extract useful time-based features\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "\n",
        "# Sort data by date to preserve time-series order\n",
        "df = df.sort_values(by='Date')\n",
        "\n",
        "# Reset index after sorting\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Final structure check\n",
        "print(\" Dataset is now analysis-ready.\\n\")\n",
        "df.info()\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prepare the dataset for analysis, I first converted the Date column into a proper datetime format. This allowed me to extract new time-based features like Year and Month, which are useful for spotting yearly or seasonal patterns in stock behavior. I then sorted the data chronologically to maintain the integrity of the time-series and reset the index for a clean structure.From the initial inspection, I found that the dataset is well-maintained,there are no missing values or duplicates. Each column holds unique and relevant information about the stock’s behavior during a specific month. These manipulations not only cleaned the data but also added useful features that will help in deeper trend analysis and model building."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')  # convert or set to NaT if bad format\n",
        "df = df.dropna(subset=['Date', 'Close'])  # drop rows where date or close is NaN"
      ],
      "metadata": {
        "id": "WN0aBPgwFHOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "\n",
        " #Line plot of Closing Price over time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(x='Date', y='Close', data=df, color='blue')\n",
        "plt.title('Yes Bank Closing Price Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a line plot because time-series data is best visualized this way to observe trends, cycles, and abrupt changes over time. It helps reveal how the stock’s closing price evolved month by month"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VPTHcJvcVVpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The line plot clearly shows periods of high volatility, sudden dips, and long-term declining or recovering trends. There might be noticeable drops around major events (like the fraud case in 2018), which can later be correlated with business context.\n",
        "\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n"
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights from this chart can have a strong positive business impact. By visualizing the trend of the closing price over time, stakeholders can identify periods of stability, volatility, or decline. It helps investors, analysts, and financial strategists recognize how the stock has responded to various internal and external factors, such as the fraud incident in 2018. This understanding can support better investment decisions, risk assessment, and strategic planning."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "jGfhKhBNWAvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there are clear signs in the chart that point toward periods of negative growth. One of the most noticeable drops in the closing price appears to align with the time around the 2018 Yes Bank fraud case. This kind of sharp decline reflects how serious external events especially those related to trust and governance can directly impact investor confidence and cause the stock value to fall. It’s a reminder that financial health isn’t the only factor influencing stock performance, public perception and credibility also play a huge role. These insights underline the importance of transparency and strong leadership in maintaining steady stock growth."
      ],
      "metadata": {
        "id": "psZdyJ9oWIAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 2: Monthly Average Closing Price (Across All Years)\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')# Ensure 'Date' column is in datetime format\n",
        "df = df.dropna(subset=['Date', 'Close'])# Drop rows where either 'Date' or 'Close' is missing\n",
        "df['Month'] = pd.to_datetime(df['Date']).dt.month\n",
        "monthly_avg = df.groupby('Month')['Close'].mean()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.lineplot(x=monthly_avg.index, y=monthly_avg.values, marker='o', color='teal')\n",
        "plt.title('Average Monthly Closing Price (Across All Years)')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Average Closing Price')\n",
        "plt.xticks(range(1, 13))\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart reveals seasonal patterns in stock price behavior by averaging closing prices across months over the years. It’s especially useful for identifying monthly investor behavior, market cycles, or specific periods of volatility or stability."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We may notice certain months (e.g., March or October) tend to have lower or higher closing prices, suggesting cyclical investment trends or reactions to fiscal year patterns, budget announcements, etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. This helps in timing decisions like selling, buying, or announcing dividends when prices are favorable. Understanding monthly behaviors helps portfolio managers align investment decisions with seasonal movements.\n",
        "\n",
        "\n",
        "If certain months consistently underperform , that indicates a market sentiment drop, possibly due to poor quarterly earnings or investor withdrawal. Recognizing this can help the company mitigate seasonal losses via better communication or strategic actions.\n",
        "\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 High vs. Low Prices over time\n",
        "# Ensure 'Date' column is in datetime format\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "# Drop rows where 'Date', 'High', or 'Low' is missing\n",
        "df = df.dropna(subset=['Date', 'High', 'Low'])\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(x='Date', y='High', data=df, label='High Price', color='green')\n",
        "sns.lineplot(x='Date', y='Low', data=df, label='Low Price', color='red')\n",
        "plt.title('High vs Low Prices of Yes Bank Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line plot compares the highest and lowest prices each month to understand the volatility range of the stock over time. It gives a clear picture of how much fluctuation there was between the peak and bottom values within each month a strong indicator of market uncertainty or stability."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot reveals periods where the gap between high and low prices widened, showing increased volatility, especially around crisis periods. Other times, the prices stayed relatively close, indicating stable market sentiment. This fluctuation often hints at how uncertain or confident investors felt during specific time frames."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definitely. Understanding volatility is crucial for risk assessment. Investors can use this insight to decide the level of risk they are comfortable with, and companies can align their investor communication strategies accordingly during high-volatility periods to reduce panic or misinformation.\n",
        "\n",
        "\n",
        "\n",
        "Yes. When the difference between high and low prices is extreme — especially when paired with an overall downward trend — it usually signals fear or confusion in the market. Such conditions can push long-term investors away and reflect weakening trust in the company’s financial or leadership stability."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 4: Box Plot of Closing Price by Year\n",
        "# Drop rows with missing dates\n",
        "# Convert 'Date' column to datetime (if not already done)\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "#  Drop rows where either 'Date' or 'Close' is missing\n",
        "df = df.dropna(subset=['Date', 'Close'])\n",
        "df.dropna(subset=['Date'], inplace=True)\n",
        "\n",
        "# Convert 'Date' column to datetime objects and extract the year\n",
        "df['Year'] = pd.to_datetime(df['Date']).dt.year\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='Year', y='Close', data=df, palette='coolwarm')\n",
        "plt.title('Distribution of Closing Prices by Year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a box plot to analyze how the closing price is distributed within each year including the median, range, and outliers. It’s one of the best visual tools for spotting yearly volatility, consistency, or unusual values in the stock price behavior."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows which years had stable closing prices and which had a wide spread or extreme values. For example, in years with financial instability or market panic, the box plot becomes tall with outliers, indicating unpredictable price movement. In more stable years, the boxes are tight and centered."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, this helps stakeholders understand how consistent the stock price was in different years. Consistency often indicates investor confidence, while volatility may signal underlying issues. This information is valuable for managing investor expectations and assessing financial performance.\n",
        "\n",
        "\n",
        "Yes, years with large spreads and many outliers suggest high uncertainty and unstable stock performance, which often deters long-term investors. For example, if one year shows a much wider box or extreme outliers downward, it could reflect market reactions to crises like fraud exposure or poor financial results."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 5: Scatter Plot of Opening Price vs. Closing Price\n",
        " #Drop rows with missing 'Open' or 'Close' values (if any)\n",
        "df = df.dropna(subset=['Open', 'Close'])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Open', y='Close', data=df, color='teal', alpha=0.6)\n",
        "plt.title('Opening Price vs. Closing Price')\n",
        "plt.xlabel('Opening Price')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BbH8_B4QC7gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A scatter plot is ideal when examining the relationship between two continuous variables. In this case, we want to understand how the stock’s opening price correlates with its closing price on the same day. It’s useful to evaluate if the stock usually rises, falls, or stays consistent during market hours.\n",
        "\n"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the plot, we observe a strong linear relationship — as the opening price increases, the closing price tends to do the same. Most points lie along the diagonal, showing minimal deviation in daily trading. However, we also see a few scattered points far from the diagonal, suggesting days where there were major price fluctuations or market reactions."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. This insight reassures investors and analysts about the stability and predictability of the stock’s daily behavior. A strong correlation implies that the stock generally closes near its opening price, reflecting a less volatile trading pattern, which is a favorable sign for risk averse investors.\n",
        "\n",
        "Yes, the scattered points, especially those far from the main trend hint at sudden intra-day market reactions, possibly driven by news, investor sentiment, or manipulation. These unpredictable shifts can indicate occasional instability, which could undermine investor confidence if such days become frequent.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Chart 6: High and Low Prices Over Time\n",
        "#Convert 'Date' to datetime (if not already)\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "df = df.dropna(subset=['Date'])  # in case any date became NaT\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(df['Date'], df['High'], label='High Price', color='green', linewidth=1.5)\n",
        "plt.plot(df['Date'], df['Low'], label='Low Price', color='red', linewidth=1.5)\n",
        "plt.title('High and Low Stock Prices Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line chart was chosen to visualize volatility over time by plotting both the high and low prices each month. It’s essential for understanding the range of price movement on a monthly basis and observing whether the gap between high and low prices has widened or narrowed during certain periods."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can clearly see periods where the difference between the high and low prices was significantly large, indicating high volatility  particularly around 2018–2020, when the bank faced regulatory and financial scrutiny. In contrast, recent years show a narrowing gap, suggesting price stabilization and possibly regained investor trust."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Understanding periods of volatility helps businesses and investors prepare better strategies. Stable price ranges often invite long-term investors, while volatile periods may be better suited for short-term trading strategies. Such insights assist in tailoring investment approaches based on risk appetite and market behavior.\n",
        "\n",
        "Yes. The wider price gaps in earlier years may reflect market uncertainty, poor investor confidence, or negative news cycles ( the Rana Kapoor case). These fluctuations could have scared off potential investors, led to a dip in the bank’s valuation, and contributed to temporary negative business growth."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Chart 7: Histogram of Closing Prices\n",
        "#Drop missing values from 'Close' if any\n",
        "df = df.dropna(subset=['Close'])\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['Close'], bins=30, color='skyblue', kde=True)\n",
        "plt.title('Distribution of Closing Prices')\n",
        "plt.xlabel('Closing Price')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram is ideal for understanding the distribution of a single numerical variable. Here, we are using it to see how frequently different ranges of closing prices occur. This helps identify central tendencies, common price levels, and any skewness in the stock's closing price history"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution is positively skewed, meaning most of the closing prices were on the lower side, with fewer high values. The peak is seen around 10rs to 30rs, which suggests that after Yes Bank’s crisis period, its stock mostly traded in a lower range. The presence of a long tail on the right reflects the earlier times when the stock was priced much higher."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. This analysis can guide investment timing and expectation-setting. For example, long-term investors can use this to understand when the stock was undervalued or overvalued. For the company, it shows how much recovery is still needed to return to its former valuation range  helping in strategic planning and stakeholder communication.\n",
        "\n",
        "Yes. The large concentration of low-priced bars shows that the stock has remained at low values for an extended period, which could indicate reduced investor confidence, past negative press, and lower market trust. Unless addressed, this perception could continue to affect long-term growth and stock performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 8: Closing Price Over Time\n",
        "# Drop rows where 'Date' or 'Close' is missing\n",
        "df = df.dropna(subset=['Date', 'Close'])\n",
        "\n",
        "# Ensure 'Date' is in datetime format\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "# Re-drop if any Date conversion fails\n",
        "df = df.dropna(subset=['Date'])\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(df['Date'], df['Close'], color='dodgerblue', linewidth=2)\n",
        "plt.title('Yes Bank Closing Price Trend Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line plot is best when trying to analyze trends over time. This chart was chosen to visualize how Yes Bank’s closing price has evolved throughout the years. It allows us to detect upward or downward trends, spikes, and crashes, which are essential for understanding historical performance and predicting future patterns."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals a sharp decline in stock price around 2018 to 2020, coinciding with the Rana Kapoor fraud case and financial instability within the bank. Post-2020, we observe a stabilization of the price at a much lower range, indicating the market is still cautious, but no longer in free fall."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. By identifying key turning points in stock price movements, business leaders and analysts can better understand investor reactions to real world events. It helps in forecasting, preparing crisis response strategies, and planning recovery measures. For investors, it's valuable for timing entry or exit based on past behavior.\n",
        "\n",
        "\n",
        "Yes. The long-term decline and flattening of the closing price suggest loss of market trust and difficulty in regaining momentum. It reflects how major controversies and poor governance can have lasting negative effects on a company’s valuation, despite later efforts at revival."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 9: Correlation Heatmap\n",
        "# Drop rows with missing values in relevant columns\n",
        "df_corr = df[['Open', 'High', 'Low', 'Close']].dropna()\n",
        "\n",
        "# Compute correlation matrix\n",
        "correlation_matrix = df_corr.corr()\n",
        "plt.figure(figsize=(10, 6))\n",
        "correlation_matrix = df[['Open', 'High', 'Low', 'Close']].corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Between Stock Price Variables')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation heatmap is excellent for understanding relationships between numerical variables. It shows how strongly variables like Open, High, Low, and Close prices are linearly related, helping identify which features might be redundant or most predictive of the closing price."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heatmap shows very strong positive correlations among all price variables, especially between Close and High (0.99) and Close and Low (0.98). This confirms that the closing price moves closely in line with the day’s high and low, making them good predictors for ML models."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Absolutely. Knowing which features are highly correlated allows us to select the most relevant variables for our model, reducing complexity and improving performance. From a business perspective, this also means better forecasting accuracy, which helps with planning and investment decisions.\n",
        "\n",
        "\n",
        "No direct insight here suggests negative growth. However, the high multicollinearity means some features might be repetitive, which if not handled correctly in modeling, can confuse interpretation and reduce generalization of the model. It’s a technical risk rather than a business one — and easily fixable.\n",
        "\n"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 10: Scatter Plot of Opening vs Closing Prices\n",
        "# Drop rows with missing values in 'Open' or 'Close'\n",
        "df_scatter = df.dropna(subset=['Open', 'Close'])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Open', y='Close', data=df, color='mediumseagreen')\n",
        "plt.title('Opening Price vs Closing Price')\n",
        "plt.xlabel('Opening Price')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A scatter plot is used to observe the relationship between two continuous variables. Here, we’re checking if there's a clear linear relationship between opening and closing prices  which can help us understand how closely they are connected day-to-day or month to month."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot shows a strong linear relationship  most points lie close to a straight line. This suggests that the opening price is a strong indicator of the closing price on a given day. However, there are a few outliers where the price significantly moved up or down by market close."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Knowing that opening price can predict closing price with high reliability makes it valuable for short-term trading strategies and algorithmic trading models. It can also help set more accurate intraday expectations and improve forecasting confidence.\n",
        "\n",
        "\n",
        "The outliers, where opening and closing prices differ heavily, could point to market volatility or sudden news impact. These represent unpredictable risks, which may cause concern for investors relying on stability indicating potential negative sentiment during certain periods."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 11: Average Monthly Closing Price\n",
        "df['Month'] = pd.to_datetime(df['Date']).dt.month_name()\n",
        "\n",
        "monthly_avg = df.groupby('Month')['Close'].mean().reindex([\n",
        "    'January', 'February', 'March', 'April', 'May', 'June',\n",
        "    'July', 'August', 'September', 'October', 'November', 'December'\n",
        "])\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=monthly_avg.index, y=monthly_avg.values, palette='viridis')\n",
        "plt.title('Average Monthly Closing Prices')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Average Closing Price')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar plot is perfect for comparing aggregated values across categories. Here, we visualize how the closing price behaves across different months — giving insights into whether any seasonal patterns exist in stock behavior.\n",
        "\n"
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We notice that February, March, and May generally show slightly lower average prices, while November and December often have higher averages. This could relate to investor activity near fiscal year-end or post festival financial optimism.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. If seasonality is consistent, businesses and investors can strategically plan buy/sell activities based on expected trends. Companies can also time announcements, offers, or stock buybacks around stronger months for maximum impact.\n",
        "\n",
        "\n",
        "Yes, if particular months (like February or March) consistently show lower values, it might reflect market uncertainty during financial reporting season or low investor confidence. Recognizing this helps the business proactively manage communication and strategy during such periods.\n",
        "\n"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 12: Rolling Mean of Closing Price (6-month window)\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df.sort_values('Date', inplace=True)\n",
        "\n",
        "df['Rolling_Mean_6M'] = df['Close'].rolling(window=6).mean()\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(df['Date'], df['Close'], label='Original Closing Price', alpha=0.4)\n",
        "plt.plot(df['Date'], df['Rolling_Mean_6M'], color='crimson', label='6-Month Rolling Mean', linewidth=2)\n",
        "plt.title('6-Month Rolling Mean of Yes Bank Closing Price')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A rolling mean (or moving average) plot is ideal for smoothing out short-term fluctuations and identifying long-term trends. It provides a clearer view of how the stock is performing over time without being distracted by random ups and downs."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 6-month rolling average line reveals that the closing price trended downward steadily during crisis periods (notably around 2018 to 2020), followed by a long flat and stable trend, suggesting the bank entered a recovery phase but has not regained momentum."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Businesses and investors can use rolling trends to time long-term decisions like when to issue stock, raise capital, or enter/exit the market. It helps identify whether recovery is real or temporary, which is vital for strategic planning.\n",
        "\n",
        "\n",
        "Yes. The prolonged flatness in the rolling mean post-crash highlights difficulty in regaining investor confidence. This stagnation points to potential weaknesses in strategy, reputation, or market conditions  insights that demand a reassessment of the company’s recovery efforts."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 13: Bar Plot of Price Range by Year\n",
        "df['Price_Range'] = df['High'] - df['Low']\n",
        "\n",
        "yearly_range = df.groupby('Year')['Price_Range'].mean()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=yearly_range.index, y=yearly_range.values, palette='magma')\n",
        "plt.title('Average Yearly Price Range (High - Low)')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Average Price Range')\n",
        "plt.grid(True, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart helps visualize market volatility by showing the average price range (difference between high and low) each year. It's especially useful to understand how volatile the stock was during specific periods, like before/during/after a financial crisis."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data shows high volatility during certain crisis years (like 2018 to 2020), with a gradually narrowing range in later years. This tells us that the stock was extremely unstable at certain times but has since become relatively less volatile, indicating potential market stabilization."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Absolutely. Understanding periods of high volatility can help businesses and investors avoid risky time frames, better manage expectations, and adjust portfolio risk levels. It also helps the bank evaluate how public perception and market events are affecting stock movement.\n",
        "\n",
        "\n",
        "Yes. The sharp spikes in the price range indicate market panic, instability, or poor sentiment, especially around known crisis years. Such instability discourages long-term investors, erodes trust, and slows down recovery, making it a clear negative insight from a business perspective."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Chart: Correlation Heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "correlation = df[['Open', 'High', 'Low', 'Close']].corr()\n",
        "\n",
        "sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title('Correlation Heatmap between Price Variables')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the correlation heatmap to understand how different price-related features  like Open, High, Low, and Close  are statistically related to one another. This kind of chart helps visually capture the strength and direction of linear relationships between variables, which is crucial before building any predictive model."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows a very high positive correlation between most of the features, especially between High and Close, Open and Close, and Low and Close. This indicates that the closing price is strongly influenced by the other price metrics meaning any model trying to predict the closing price can confidently include these variables as inputs."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart: Pair Plot of Price Variables\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.pairplot(df[['Open', 'High', 'Low', 'Close']])\n",
        "plt.suptitle('Pair Plot of Price Variables', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked the pair plot because it’s a great way to explore the pairwise relationships between multiple numerical variables in one view. It helps in visually identifying linear trends, clusters, and potential outliers across all combinations of variables like Open, High, Low, and Close. This is especially useful before applying regression models to see how variables interact with each other.\n",
        "\n"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pair plot reveals that the features Open, High, Low, and Close have strong linear relationships, as shown by the tight clustering along diagonals in their scatter plots. It also confirms that High and Close or Low and Close are highly aligned, indicating that daily price fluctuations move together consistently. The distribution plots on the diagonal also show that Close prices are slightly skewed, which might need transformation for certain models."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the charts and data analysis, I’ve defined three hypotheses to validate insights using statistical testing:\n",
        "\n",
        "1. Do average closing prices change across years?\n",
        "There seemed to be yearly variation in closing prices. I’ll test whether these differences are statistically significant using One-Way ANOVA.\n",
        "\n",
        "2. Is there a significant difference between High and Low prices?\n",
        "Though the High and Low prices move closely, I’ll use a Paired t-test to check if their averages are meaningfully different.\n",
        "\n",
        "3. Does trading volume affect closing price?\n",
        "To verify the link between volume and closing price, I’ll use Pearson correlation to test if a significant relationship exists.\n",
        "\n"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothetical Statement 1\n",
        "1. State your research hypothesis:\n",
        "Null Hypothesis: There is no significant difference in the average closing price of Yes Bank stock across different years.\n",
        "\n",
        "Alternate Hypothesis: There is a significant difference in the average closing price across different years."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/current data_YesBank_StockPrices - data_YesBank_StockPrices.csv')\n",
        "\n",
        "# Just in case there are any hidden spaces in column names\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Extract year from the FormattedDate column\n",
        "df['Year'] = pd.to_datetime(df['FormattedDate']).dt.year\n",
        "\n",
        "# Run one-way ANOVA on Close prices grouped by year\n",
        "anova_data = [group['Close'].values for name, group in df.groupby('Year') if len(group) > 1]\n",
        "\n",
        "f_stat, p_value = stats.f_oneway(*anova_data)\n",
        "\n",
        "print(\"F-statistic:\", f_stat)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the ANOVA (Analysis of Variance) test to get the p-value."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose ANOVA because I needed to compare the average closing stock prices across different years. ANOVA is perfect when you're checking if three or more groups (in this case, years) have different means."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Null Hypothesis:\n",
        "The distribution of closing stock prices is the same across all years.\n",
        "\n",
        "Alternate Hypothesis:\n",
        "At least one year has a different distribution of closing stock prices."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import kruskal\n",
        "\n",
        "# Group closing prices by year\n",
        "grouped_data = [group[\"Close\"].values for _, group in df.groupby(\"Year\")]\n",
        "\n",
        "# Perform Kruskal-Wallis Test\n",
        "stat, p_value = kruskal(*grouped_data)\n",
        "\n",
        "print(\"Kruskal-Wallis H-statistic:\", stat)\n",
        "print(\"P-value:\", p_value)\n",
        "\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I performed the Kruskal-Wallis H-Test, a non-parametric test used to compare multiple independent groups."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the Kruskal-Wallis test because it doesn't assume the data follows a normal distribution.\n",
        "It's a good fit when comparing stock closing prices across multiple years, especially if the data may have outliers or non-normal patterns."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis:\n",
        "There is no monotonic relationship between the Open and Close prices.\n",
        "\n",
        "Alternate Hypothesis:\n",
        "There is a monotonic relationship between the Open and Close prices."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Apply Spearman's rank correlation test\n",
        "stat, p_value = spearmanr(df['Open'], df['Close'])\n",
        "\n",
        "print(\"Spearman correlation coefficient:\", stat)\n",
        "print(\"P-value:\", p_value)\n",
        "\n"
      ],
      "metadata": {
        "id": "qodiC1UikrNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the Spearman Rank Correlation Test.\n",
        "\n"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because Spearman’s test is ideal for checking whether two variables (like Open and Close) have a monotonic (consistently increasing or decreasing) relationship, without assuming a linear trend or normal distribution."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing values in each column:\\n\", missing_values)\n",
        "\n",
        "# Example 1: Fill missing numerical values with mean\n",
        "df['Close'] = df['Close'].fillna(df['Close'].mean())\n",
        "\n",
        "# Example 2: Forward fill for time-series continuity\n",
        "df = df.fillna(method='ffill')\n",
        "\n",
        "# If any still left, use backward fill\n",
        "df = df.fillna(method='bfill')\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used total three imputation techniques, they are\n",
        "\n",
        "Mean Imputation – For columns like 'Close', I used the mean of the column to fill missing values. because it prevents skewing the data and is simple for continuous numerical features.\n",
        "\n",
        "Forward Fill (ffill) – Since this is a time-series stock dataset, I used forward fill to propagate the last known value forward which maintains the continuity of stock trends.\n",
        "\n",
        "Backward Fill (bfill) – To ensure no missing data remains after forward fill, I used backward fill as a fallback method. This is what is useful when missing values occur at the start of the dataset."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualizing outliers using boxplot\n",
        "sns.boxplot(df['Close'])\n",
        "plt.title(\"Boxplot - Close Prices\")\n",
        "plt.show()\n",
        "\n",
        "# Treating outliers using IQR method\n",
        "Q1 = df['Close'].quantile(0.25)\n",
        "Q3 = df['Close'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define bounds\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Capping outliers to the bounds\n",
        "df['Close'] = np.where(df['Close'] > upper_bound, upper_bound,\n",
        "               np.where(df['Close'] < lower_bound, lower_bound, df['Close']))\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the Interquartile Range (IQR) method for identifying and treating outliers.\n",
        "\n",
        "First, I visualized the outliers using a boxplot to understand their presence and distribution.\n",
        "\n",
        "Then, I calculated Q1 and Q3 (25th and 75th percentiles) to compute the IQR.\n",
        "\n",
        "Any value beyond 1.5 × IQR from Q1 or Q3 was considered an outlier.\n",
        "\n",
        "Instead of dropping them, I applied capping replacing extreme values with upper or lower threshold limits to retain the data while reducing skewness."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List categorical columns\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "print(\"Categorical columns found:\", categorical_cols)\n",
        "if len(categorical_cols) > 0:\n",
        "    df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
        "else:\n",
        "    df_encoded = df.copy()  # No encoding needed\n",
        "    print(\"No categorical columns found. Proceeding without encoding.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The only object-type columns in the dataset were 'Date' and 'FormattedDate', which are temporal in nature. Instead of categorical encoding, I extracted meaningful features such as Year, Month, and Day to make the date information more usable for analysis and modeling.\n",
        "\n",
        "This approach preserves the time-based patterns in the data and avoids inappropriate one-hot encoding of continuous date values."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "import pandas as pd\n",
        "\n",
        "# Convert 'Date' column to datetime with the correct format\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%b-%y', errors='coerce')\n",
        "\n",
        "# Extract time-based features\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Day'] = df['Date'].dt.day\n",
        "df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "\n",
        "# Create price-based derived features\n",
        "df['PriceRange'] = df['High'] - df['Low']\n",
        "df['DailyChange'] = df['Close'] - df['Open']\n",
        "df['Volatility'] = (df['High'] - df['Low']) / df['Open']\n",
        "\n",
        "# Display the first few rows with the new features\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "\n",
        "# Define features and target\n",
        "X = df[['Open', 'High', 'Low', 'Month', 'Day', 'DayOfWeek', 'PriceRange', 'DailyChange', 'Volatility']]\n",
        "y = df['Close']\n",
        "\n",
        "# Apply SelectKBest with f_regression\n",
        "selector = SelectKBest(score_func=f_regression, k=5)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "# Get selected feature names\n",
        "selected_features = X.columns[selector.get_support()]\n",
        "print(\"Selected Features:\", selected_features.tolist())"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used SelectKBest with the F-test (f_regression) as the feature selection method.\n",
        "This method evaluates the relationship between each input feature and the target variable (Close price), selecting the top features with the highest statistical significance.\n",
        "\n",
        "I chose this method because it is:\n",
        "\n",
        "Simple yet effective for numerical data.\n",
        "\n",
        "Helps in reducing dimensionality.\n",
        "\n",
        "Prevents overfitting by removing less relevant features.\n",
        "\n",
        "Improves model performance by keeping only statistically important inputs."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying SelectKBest, the following features were found to be most important:\n",
        "\n",
        "Open – Strongly correlates with the closing price; it's a direct indicator of the day’s market trend.\n",
        "\n",
        "High – Shows the highest price reached in a day; often useful in predicting end-of-day movements.\n",
        "\n",
        "Low – Like High, it reflects price fluctuation and helps in understanding volatility.\n",
        "\n",
        "DailyChange – Engineered feature (High - Low) that captures intraday price variation.\n",
        "\n",
        "Volatility – Another engineered feature (DailyChange / Open), useful for understanding market risk.\n"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, data transformation was necessary in this project to improve the performance and accuracy of the model."
      ],
      "metadata": {
        "id": "_0t2zvAWM1pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df[['Open', 'High', 'Low', 'Close']])\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df[['Open', 'High', 'Low', 'Close']])\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the StandardScaler (Z-score Normalization) method to scale the data.\n",
        "\n",
        "My dataset contains numerical features like Open, High, Low, and Close with different ranges and units.\n",
        "\n",
        "StandardScaler transforms the data to have zero mean and unit variance.\n",
        "\n",
        "This scaling helps algorithms that are sensitive to feature magnitude (like Linear Regression, SVM, and K-Means) to perform better and converge faster.\n",
        "\n",
        "It ensures that no feature dominates the others simply due to scale."
      ],
      "metadata": {
        "id": "qmTfeIj0NN_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, dimensionality reduction is useful in this project to:\n",
        "\n",
        "Remove redundant or highly correlated features that do not contribute significantly to the prediction.\n",
        "\n",
        "Improve model performance by reducing noise and overfitting.\n",
        "\n",
        "Speed up computation time by lowering the number of features the model processes.\n",
        "\n",
        "Make the data visualization and interpretation easier when projecting high-dimensional data to 2 or 3 dimensions.\n",
        "\n",
        "Even though the dataset has only a few numerical features (Open, High, Low, Close), dimensionality reduction like PCA (Principal Component Analysis) was used to check if the essential information can be retained in fewer dimensions, which can benefit downstream models."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Select numerical features for PCA\n",
        "features = ['Open', 'High', 'Low', 'Close']\n",
        "X = df[features]\n",
        "\n",
        "# Step 1: Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 2: Apply PCA\n",
        "pca = PCA(n_components=2)  # Reduce to 2 components for visualization\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Step 3: Create a new DataFrame with principal components\n",
        "pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
        "\n",
        "# Optional: Add Date column for tracking\n",
        "pca_df['Date'] = df['Date'].values\n",
        "\n",
        "# Explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(\"Explained variance by components:\", explained_variance)\n",
        "\n",
        "# View the transformed dataset\n",
        "pca_df.head()\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Principal Component Analysis (PCA) as the dimensionality reduction technique.Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assume X contains features and y contains target variable\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used an 80:20 split — 80% for training and 20% for testing. It's a common and reliable choice that balances learning and evaluation well."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, we don’t need to handle imbalance because the dataset is continuous in nature, it deals with stock prices rather than categorical classes. Imbalance handling is only necessary when we have classification problems with uneven class distribution."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1{Linear Regression} Implementation\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Selecting features and target\n",
        "X = df[['Open', 'High', 'Low']]\n",
        "y = df['Close']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the Algorithm\n",
        "model_1 = LinearRegression()\n",
        "model_1.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_1 = model_1.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_1 = mean_squared_error(y_test, y_pred_1)\n",
        "r2_1 = r2_score(y_test, y_pred_1)\n",
        "\n",
        "print(\"Model 1 - Linear Regression Results:\")\n",
        "print(f\"Mean Squared Error: {mse_1}\")\n",
        "print(f\"R² Score: {r2_1}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Store metrics\n",
        "metrics = {'Mean Squared Error': mse_1, 'R2 Score': r2_1}\n",
        "\n",
        "# Create bar chart\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(metrics.keys(), metrics.values(), color=['skyblue', 'orange'])\n",
        "plt.title('Evaluation Metrics for Model 1 - Linear Regression')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, max(metrics.values()) * 1.2)\n",
        "\n",
        "# Add values on top\n",
        "for i, v in enumerate(metrics.values()):\n",
        "    plt.text(i, v + 0.01, f\"{v:.4f}\", ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Define the model (Ridge Regression adds regularization)\n",
        "ridge = Ridge()\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "param_grid = {\n",
        "    'alpha': [0.01, 0.1, 1, 10, 100]  # Ridge regularization strength\n",
        "}\n",
        "\n",
        "# GridSearchCV setup\n",
        "grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid, cv=5, scoring='r2')\n",
        "\n",
        "# Fit the model on training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_ridge = grid_search.best_estimator_\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_ridge = best_ridge.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
        "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
        "\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Mean Squared Error:\", mse_ridge)\n",
        "print(\"R² Score:\", r2_ridge)\n",
        "print(\"Model Accuracy (based on R²):\", r2_ridge* 100, \"%\")\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV for hyperparameter optimization. It exhaustively searches through a specified range of hyperparameter values,it’s simple and reliable when the parameter space is not too large,it performs cross-validation automatically, helping reduce overfitting"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there was a slight improvement after hyperparameter tuning.\n",
        "The R² score increased from 0.983544389505753 to 0.9835443919282351 (a tiny decimal change), and the Mean Squared Error slightly decreased from 137.2334 to 137.2333.\n",
        "While the change is minimal, it still indicates that the model became a bit more precise with the optimized parameters."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Used: Random Forest Regressor.\n",
        "Random Forest is a powerful ensemble method that builds multiple decision trees and combines their outputs to improve accuracy and avoid overfitting."
      ],
      "metadata": {
        "id": "mpxYHphIvvwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Random Forest Regressor Results:\")\n",
        "print(f\"Mean Squared Error: {mse_rf}\")\n",
        "print(f\"R² Score: {r2_rf}\")\n",
        "print(\"Model Accuracy (based on R²):\",r2_rf * 100, \"%\")\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(['MSE', 'R² Score'], [mse_rf, r2_rf], color=['skyblue', 'lightgreen'])\n",
        "plt.title('Evaluation Metrics - Random Forest Regressor')\n",
        "plt.ylabel('Score')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Define the model\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "# GridSearch with Cross-Validation\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best Model\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Predict on Test Data\n",
        "y_pred_rf_tuned = best_rf.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "mse_rf_tuned = mean_squared_error(y_test, y_pred_rf_tuned)\n",
        "r2_rf_tuned = r2_score(y_test, y_pred_rf_tuned)\n",
        "\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"After Tuning - Mean Squared Error:\", mse_rf_tuned)\n",
        "print(\"After Tuning - R² Score:\", r2_rf_tuned)\n",
        "print(\"Model Accuracy (based on R²):\",r2_rf_tuned * 100, \"%\")\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I applied GridSearchCV to find the best combination of parameters for the Random Forest model.\n",
        "It helped fine-tune the model to reduce error and improve accuracy through cross-validation."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there is an improvement after tuning!\n",
        "The Mean Squared Error (MSE) dropped from 171.88 to 159.00, and the R² Score improved from 0.9794 to 0.9809.\n",
        "This means the tuned model predicts better and explains more variance in the data."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Squared Error (MSE)\n",
        " Indicates the average squared difference between actual and predicted values.\n",
        " Lower MSE means our predictions are closer to real stock prices.\n",
        " Business Impact: Reduces forecasting errors, crucial in making informed decisions like when to buy/sell stocks.\n",
        "\n",
        "R2 Score\n",
        " Measures how well the model explains the variability in the data.\n",
        " Closer to 1 means better performance.\n",
        " Business Impact: A high R2 (like 0.9809) shows the model reliably captures market behavior — boosting confidence in automation, planning, and risk assessment."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3: XGBoost Regressor\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Create the model\n",
        "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(\"XGBoost Regressor Results:\")\n",
        "print(\"Mean Squared Error:\", mse_xgb)\n",
        "print(\"R² Score:\", r2_xgb)\n",
        "print(\"Model Accuracy (based on R²):\",r2_xgb * 100, \"%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scores from XGBoost model\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "# Bar chart to visualize performance\n",
        "metrics = ['Mean Squared Error', 'R² Score']\n",
        "scores = [mse_xgb, r2_xgb]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "bars = plt.bar(metrics, scores, color=['skyblue', 'lightgreen'])\n",
        "plt.title(\"XGBoost Model Evaluation Metrics\")\n",
        "plt.ylabel(\"Score\")\n",
        "\n",
        "# Add text labels on top of bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, height, f'{height:.4f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'subsample': [0.8, 1]\n",
        "}\n",
        "\n",
        "# Create the base model\n",
        "xgb = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# Grid Search with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid,\n",
        "                           cv=5, scoring='r2', n_jobs=-1, verbose=1)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Predict using best model\n",
        "best_xgb = grid_search.best_estimator_\n",
        "y_pred_best_xgb = best_xgb.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "mse_best_xgb = mean_squared_error(y_test, y_pred_best_xgb)\n",
        "r2_best_xgb = r2_score(y_test, y_pred_best_xgb)\n",
        "\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"After Tuning - Mean Squared Error:\", mse_best_xgb)\n",
        "print(\"After Tuning - R² Score:\", r2_best_xgb)\n",
        "print(\"Model Accuracy (based on R²):\",r2_xgb * 100, \"%\")\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV because it systematically checks all combinations of given hyperparameters.It’s easy to implement, reliable, and works well when the dataset isn’t extremely large, helping to find the best set of parameters to boost model accuracy and generalization."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, there was no improvement after hyperparameter tuning."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Mean Squared Error (MSE) and R2 Score as key metrics.\n",
        "\n",
        "MSE shows how much the predictions deviate from the actual stock prices, lower is better, meaning more accurate predictions.\n",
        "\n",
        "R2 Score tells how well the model explains the variance in the data, closer to 1 means better prediction power."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the Linear Regression Model (after hyperparameter tuning) as the final model.\n",
        "\n",
        "It gave the lowest MSE and highest R2 Score among all models.\n",
        "\n",
        "It also performed consistently, with very little change after tuning indicating robustness and simplicity.\n",
        "\n",
        "For business, a simpler, stable, and interpretable model like Linear Regression is often preferable over complex ones with marginal improvement."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Linear Regression, a straightforward and interpretable algorithm.\n",
        "\n",
        "To explain feature importance, I used model coefficients\n",
        "\n",
        "Features with higher absolute coefficient values have more impact on the prediction.For example, if Open and High prices had high positive coefficients, it means they strongly drive the predicted Close price."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import pickle\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# (Optional) Split data if not already split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Linear Regression model\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_train, y_train)\n",
        "\n",
        "# Save the model to a pickle file\n",
        "with open('best_model.pkl', 'wb') as file:\n",
        "    pickle.dump(linear_model, file)\n",
        "\n",
        "print(\" Model successfully saved as 'best_model.pkl'\")\n",
        "\n",
        "# To load it later:\n",
        "# with open('best_model.pkl', 'rb') as file:\n",
        "#     loaded_model = pickle.load(file)\n",
        "\n"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File\n",
        "import pickle\n",
        "\n",
        "# Load the saved model from the pickle file\n",
        "with open('best_model.pkl', 'rb') as file:\n",
        "    loaded_model = pickle.load(file)\n",
        "\n",
        "# Sanity Check: Predict on unseen test data\n",
        "# Make sure X_test is already defined in your environment\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Show a few predictions\n",
        "print(\" Sanity check - Sample predictions:\")\n",
        "print(y_pred[:5])  # show first 5 predictions\n"
      ],
      "metadata": {
        "id": "c_8MGRVo7FqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare predictions with actual target values\n",
        "print(\"Predicted:\", y_pred[:5])\n",
        "print(\"Actual:   \", y_test[:5].values)"
      ],
      "metadata": {
        "id": "Xy0ec5X47U7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we successfully built and evaluated multiple machine learning models to predict stock prices using historical market data. We began with thorough data preprocessing, including handling missing values, outliers, and feature scaling. Feature engineering and selection helped enhance model performance while preventing overfitting.\n",
        "\n",
        "Three machine learning models were implemented ,Linear Regression, Random Forest Regressor, and XGBoost Regressor. After performing hyperparameter tuning and cross-validation, the Linear Regression model showed the best performance with an R² score of 0.9835, making it the final chosen model.\n",
        "\n",
        "We saved this model using pickle and validated it by predicting unseen test data to ensure correctness. The model produced predictions closely aligned with actual values, indicating strong reliability and low error.\n",
        "Overall, this solution demonstrates a robust, scalable approach for stock price forecasting, with potential for positive business impact in financial decision-making, investment planning, and market analysis."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}